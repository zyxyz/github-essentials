{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -v -p sklearn,numpy,scipy,matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from preamble import *\n",
    "plt.rcParams['image.cmap'] = \"gray\"\n",
    "plt.rcParams['axes.xmargin'] = 0.05\n",
    "plt.rcParams['axes.ymargin'] = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 비지도 학습과 데이터 전처리\n",
    "### 비지도 학습의 종류\n",
    "  * 비지도 변환(Unsupervised Transformation)\n",
    "    - 데이터를 새롭게 표현하여 사람이나 다른 머신러닝 알고리즘이 원래 데이터보다 쉽게 해설할 수 있도록 만드는 알고리즘\n",
    "    - 차원 축소(Dimensionality Reduction)\n",
    "      - 특징이 많은 고차원 데이터의 특징을 줄이면서 꼭 필요한 특징을 포함한 데이터로 표현하는 방법\n",
    "      - 예) 시각화를 위해 데이터셋을 2차원으로 변경\n",
    "    - 데이터를 구성하는 단위나 성분 찾기\n",
    "      - 예) 많은 텍스트 문서에서 주제를 추출\n",
    "      - 예) 소셜 미디어에서 이뤄지는 대화 주제 확인\n",
    "  * 군집(Clustering)\n",
    "    - 데이터를 비슷한 것끼리 그룹으로 묶는 것\n",
    "      - 예) 소셜 미디어에 업로드한 사진 분류\n",
    "  \n",
    "### 비지도 학습의 도전 과제\n",
    "  * 평가 방법\n",
    "    - 레이블이 없는 데이터 사용 ==> 올바른 출력을 모르는 상태에서 학습 진행\n",
    "    \n",
    "  * 탐색적 분석 단계에서 많이 사용\n",
    "  * 지도 학습의 전처리 단계에서 사용\n",
    "  \n",
    "### 데이터 전처리와 스케일 조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "mglearn.plots.plot_scaling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 여러가지 전처리 방법\n",
    "  * StandardScaler\n",
    "    - 모든 특징들이 같은 스케일을 갖게 함\n",
    "    - 평균 0, 분산 1\n",
    "    - 최대, 최소를 제한하지 않음\n",
    "  * RobustScaler\n",
    "    - 모든 특징들이 같은 스케일을 갖게 함\n",
    "    - 중간 값, 사분위 값\n",
    "    - 이상치에 영향을 받지 않음\n",
    "  * MinMaxScaler\n",
    "    - 모든 특성이 [0, 1]에 위치하도록\n",
    "  * Normalizer\n",
    "    - 특징 벡터의 유클리드 길이가 1이 되도록\n",
    "    - 지름이 1인 원(구)에 데이터 포인트를 투영\n",
    "    - 특징 벡터의 길이보다 방향(각도)가 중요할 때\n",
    "    \n",
    "#### 데이터 변환 적용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target,\n",
    "                                                    random_state=1)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 변환\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "# 스케일이 조정된 후 데이터셋의 속성을 출력합니다\n",
    "print(\"변환된 후 크기: {}\".format(X_train_scaled.shape))\n",
    "print(\"스케일 조정 전 특성별 최소값:\\n {}\".format(X_train.min(axis=0)))\n",
    "print(\"스케일 조정 전 특성별 최대값:\\n {}\".format(X_train.max(axis=0)))\n",
    "print(\"스케일 조정 후 특성별 최소값:\\n {}\".format(X_train_scaled.min(axis=0)))\n",
    "print(\"스케일 조정 후 특성별 최대값:\\n {}\".format(X_train_scaled.max(axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 테스트 데이터 변환\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "# 스케일이 조정된 후 테스트 데이터의 속성을 출력합니다\n",
    "print(\"스케일 조정 후 특성별 최소값:\\n{}\".format(X_test_scaled.min(axis=0)))\n",
    "print(\"스케일 조정 후 특성별 최대값:\\n{}\".format(X_test_scaled.max(axis=0)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "* 테스트 셋의 최솟값과 최댓값이 0과 1이 아님에 주의!\n",
    "  오히려 0과 1의 범위를 벗어나기도 함\n",
    "  \n",
    "* 훈련 셋과 테스트 셋에 같은 변환을 적용해야 함\n",
    "  : 테스트 셋을 변환할 때 훈련 셋 데이터에 기반한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 동일한 방법으로 훈련 데이터와 테스트 데이터의 스케일을 조정하기\n",
    "  * 훈련 데이터와 테스트 데이터의 스케일 조정을 함께 한 경우와 따로 한 경우 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "# 인위적인 데이터셋 생성\n",
    "X, _ = make_blobs(n_samples=50, centers=5, random_state=4, cluster_std=2)\n",
    "# 훈련 세트와 테스트 세트로 나눕니다\n",
    "X_train, X_test = train_test_split(X, random_state=5, test_size=.1)\n",
    "\n",
    "# 훈련 세트와 테스트 세트의 산점도를 그립니다\n",
    "fig, axes = plt.subplots(1, 3, figsize=(13, 4))\n",
    "axes[0].scatter(X_train[:, 0], X_train[:, 1],\n",
    "                c=mglearn.cm2(0), label=\"훈련 세트\", s=60)\n",
    "axes[0].scatter(X_test[:, 0], X_test[:, 1], marker='^',\n",
    "                c=mglearn.cm2(1), label=\"테스트 세트\", s=60)\n",
    "axes[0].legend(loc='upper left')\n",
    "axes[0].set_title(\"원본 데이터\")\n",
    "\n",
    "# MinMaxScaler를 사용해 스케일을 조정합니다\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 스케일이 조정된 데이터의 산점도를 그립니다\n",
    "axes[1].scatter(X_train_scaled[:, 0], X_train_scaled[:, 1],\n",
    "                c=mglearn.cm2(0), label=\"훈련 세트\", s=60)\n",
    "axes[1].scatter(X_test_scaled[:, 0], X_test_scaled[:, 1], marker='^',\n",
    "                c=mglearn.cm2(1), label=\"테스트 세트\", s=60)\n",
    "axes[1].set_title(\"스케일 조정된 데이터\")\n",
    "\n",
    "# 테스트 세트의 스케일을 따로 조정합니다\n",
    "# 테스트 세트의 최솟값은 0, 최댓값은 1이 됩니다\n",
    "# 이는 예제를 위한 것으로 절대로 이렇게 사용해서는 안됩니다\n",
    "test_scaler = MinMaxScaler()\n",
    "test_scaler.fit(X_test)\n",
    "X_test_scaled_badly = test_scaler.transform(X_test)\n",
    "\n",
    "# 잘못 조정된 데이터의 산점도를 그립니다\n",
    "axes[2].scatter(X_train_scaled[:, 0], X_train_scaled[:, 1],\n",
    "                c=mglearn.cm2(0), label=\"training set\", s=60)\n",
    "axes[2].scatter(X_test_scaled_badly[:, 0], X_test_scaled_badly[:, 1],\n",
    "                marker='^', c=mglearn.cm2(1), label=\"test set\", s=60)\n",
    "axes[2].set_title(\"잘못 조정된 데이터\")\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xlabel(\"특성 0\")\n",
    "    ax.set_ylabel(\"특성 1\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "* 첫 번째와 두 번째 그래프\n",
    "  - 같은 MinMaxScaler로 스케일 조정\n",
    "  - 축의 눈금만 바뀌었을 뿐 동일한 그래프\n",
    "  - 훈련 데이터의 모든 특징은 0과 1 사이에 존재\n",
    "  - 테스트 데이터의 최솟값과 최댓값은 0과 1이 아님\n",
    "  \n",
    "* 첫 번째와 세 번째 그래프\n",
    "  - 훈련 셋과 테스트 셋을 서로 다른 방식으로 스케일 조정\n",
    "  - 훈련 셋과 테스트 셋 모두 최솟값과 최댓값이 0과 1\n",
    "  - 서로 다른 그래프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# 메소드 체이닝(chaining)을 사용하여 fit과 transform을 연달아 호출합니다\n",
    "X_scaled = scaler.fit(X_train).transform(X_train)\n",
    "# 위와 동일하지만 더 효율적입니다\n",
    "X_scaled_d = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 지도 학습에서 데이터 전처리 효과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target,\n",
    "                                                    random_state=0)\n",
    "\n",
    "svm = SVC(C=100)\n",
    "svm.fit(X_train, y_train)\n",
    "print(\"테스트 세트 정확도: {:.2f}\".format(svm.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0~1 사이로 스케일 조정\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 조정된 데이터로 SVM 학습\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 스케일 조정된 테스트 세트의 정확도\n",
    "print(\"스케일 조정된 테스트 세트의 정확도: {:.2f}\".format(svm.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평균 0, 분산 1을 갖도록 스케일 조정\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 조정된 데이터로 SVM 학습\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 스케일 조정된 테스트 세트의 정확도\n",
    "print(\"SVM test accuracy: {:.2f}\".format(svm.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 차원 축소, 특성 추출, 매니폴드 학습"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "* 비지도 학습으로 데이터를 변환하는 이유\n",
    "  - 시각화\n",
    "  - 데이터 압축\n",
    "  - 추가적인 처리 -> 지도 학습에 활용 등"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 주성분 분석 (PCA, Principal Component Analysis)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "* 특징징들이 통계적으로 상관관계가 없도록 데이터 셋을 회전시키는 기술\n",
    "* 회전한 뒤 데이터 설명에 얼마나 중요한가에 따라 새로운 특성 중 일부만 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mglearn.plots.plot_pca_illustration()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# PCA 알고리즘\n",
    "  1. 원본 데이터에서 분산이 가장 큰 방향 '성분 1'을 찾음 (첫 번째 그림)\n",
    "    - 이 방향(벡터)가 데이터에서 가장 많은 정보를 담고 있는 방향\n",
    "    - 특징들의 상관관계가 가장 큰 방향\n",
    "    \n",
    "  2. '성분 1'방향과 직각인 방향 중 가장 많은 정보를 담은 방향 찾음\n",
    "    - 2차원: 직각 방향 하나\n",
    "    - 고차원: (무한히) 많은 직각 방향 존재\n",
    "    \n",
    "  3. 주성분\n",
    "    - 위 과정을 거쳐 찾은 방향\n",
    "      : 데이터에 있는 주된 분산의 방향\n",
    "    - 일반적으로 원본 특징 개수만큼의 주성분 존재\n",
    "    \n",
    "  4. 주성분 1과 2를 x축과 y축에 나란하도록 회전\n",
    "    - 회전하기 전 데이터에서 평균을 빼서 중심을 원점에 맞춤\n",
    "    - PCA에 의해 회전된 두 축은 연관되어 있지 않으므로 변환된 데이터의 상관관계 행렬이 대각 성분을 제외하고는 0, 대각 성분은 1\n",
    "\n",
    "  5. 세 번째 그래프  # PCA를 차원 축소 용도로 사용\n",
    "    - 첫 번째 주성분만 유지\n",
    "    - 2차원 데이터 셋 --> 1차원 데이터 셋 (차원 감소)\n",
    "    - 가장 유용한 방향을 찾아서 그 방향의 성분\n",
    "    \n",
    "  6. 데이터에 다시 평균 더해서 반대로 회전\n",
    "    - 첫 번째 주성분의 정보만 담고 있음\n",
    "    \n",
    "# PCA 용도\n",
    "  * 차원 축소\n",
    "  * 노이즈 제거\n",
    "  * 주성분 정보 시각화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 시각화를 위해 유방암 데이터셋에 PCA 적용하기"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "* 고차원 데이터 셋의 시각화에 PCA 적용\n",
    "\n",
    "* 유방암 데이터 셋\n",
    "  - 30개의 특징, 무수히 많은 산점도 --> 활용 불가\n",
    "  - 양성, 악성 두 클래스에 대한 각 특징 별 히스토그램을 그려서 데이터 이해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(15, 2, figsize=(10, 20))\n",
    "malignant = cancer.data[cancer.target == 0]\n",
    "benign = cancer.data[cancer.target == 1]\n",
    "\n",
    "ax = axes.ravel()\n",
    "\n",
    "for i in range(30):\n",
    "    _, bins = np.histogram(cancer.data[:, i], bins=50)\n",
    "    ax[i].hist(malignant[:, i], bins=bins, color=mglearn.cm3(0), alpha=.5)\n",
    "    ax[i].hist(benign[:, i], bins=bins, color=mglearn.cm3(2), alpha=.5)\n",
    "    ax[i].set_title(cancer.feature_names[i])\n",
    "    ax[i].set_yticks(())\n",
    "ax[0].set_xlabel(\"특성 크기\")\n",
    "ax[0].set_ylabel(\"빈도\")\n",
    "ax[0].legend([\"악성\", \"양성\"], loc=\"best\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "* 상위 그래프는 특징 간 상호작용, 상호작용이 클래스와 어떤 관계가 있는지 알져주지 못함\n",
    "  - PCA 사용, 주요 상호작용을 찾아 더 나은 그래프 제공"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "* PCA 적용 전 데이터 스케일 조정\n",
    "  - 각 특징의 분산이 1이 되도록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(cancer.data)\n",
    "X_scaled = scaler.transform(cancer.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer.target_names"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "* scikit-learn에서 PCA 적용\n",
    "  1. PCA 객체 생성\n",
    "  2. fit 메서드 호출 -> 주성분 찾기\n",
    "  3. transform 메서드 호출 -> 데이터 회전, 차원 축소\n",
    "    - 차원 축소시 유지할 성분 개수를 알려주어야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# 데이터의 처음 두 개 주성분만 유지시킵니다\n",
    "pca = PCA(n_components=2)\n",
    "# 유방암 데이터로 PCA 모델을 만듭니다\n",
    "pca.fit(X_scaled)\n",
    "\n",
    "# 처음 두 개의 주성분을 사용해 데이터를 변환합니다\n",
    "X_pca = pca.transform(X_scaled)\n",
    "print(\"원본 데이터 형태: {}\".format(str(X_scaled.shape)))\n",
    "print(\"축소된 데이터 형태: {}\".format(str(X_pca.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클래스를 색깔로 구분하여 처음 두 개의 주성분을 그래프로 나타냅니다.\n",
    "plt.figure(figsize=(8, 8))\n",
    "mglearn.discrete_scatter(X_pca[:, 0], X_pca[:, 1], cancer.target)\n",
    "plt.legend([\"악성\", \"양성\"], loc=\"best\")\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "plt.xlabel(\"첫 번째 주성분\")\n",
    "plt.ylabel(\"두 번째 주성분\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 해석\n",
    "* 두 주성분으로 구성한 산점도에서 두 클래스가 잘 구분되는 것 확인\n",
    "  -> 선형분류기로도 좋은 성능 가능성\n",
    "* 악성 포인트가 양성 포인트보다 더 넓게 퍼져 있음"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "* components_ 속성\n",
    "  : 학습 후 주성분이 저장됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PCA 주성분 형태: {}\".format(pca.components_.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PCA 주성분: {}\".format(pca.components_))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 해석\n",
    "* 첫 번째 주성분\n",
    "  : 모든 특징의 부호가 같다.\n",
    "    - 주성분의 화살표 방향은 의미 없으나\n",
    "    - 모든 특징 사이에 공통의 상호관계가 있다는 뜻\n",
    "    - 한 특징의 값이 커지면 다른 특징의 값도 커진다\n",
    "    \n",
    "* 두 주성분 모두 30개의 특성 값이 존재하므로 그래프의 두 축을 설명하기가 쉽지 않다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(pca.components_, cmap='viridis')\n",
    "plt.yticks([0, 1], [\"첫 번째 주성분\", \"두 번째 주성분\"])\n",
    "plt.colorbar()\n",
    "plt.xticks(range(len(cancer.feature_names)),\n",
    "           cancer.feature_names, rotation=60, ha='left')\n",
    "plt.xlabel(\"특성\")\n",
    "plt.ylabel(\"주성분\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 고유얼굴 특성 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_lfw_people\n",
    "people = fetch_lfw_people(min_faces_per_person=20, resize=0.7)\n",
    "image_shape = people.images[0].shape\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 8),\n",
    "                         subplot_kw={'xticks': (), 'yticks': ()})\n",
    "for target, image, ax in zip(people.target, people.images, axes.ravel()):\n",
    "    ax.imshow(image)\n",
    "    ax.set_title(people.target_names[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.target[0:10], people.target_names[people.target[0:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"people.images.shape: {}\".format(people.images.shape))\n",
    "print(\"클래스 개수: {}\".format(len(people.target_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 타깃이 나타난 횟수 계산\n",
    "counts = np.bincount(people.target)\n",
    "# 타깃별 이름과 횟수 출력\n",
    "for i, (count, name) in enumerate(zip(counts, people.target_names)):\n",
    "    print(\"{0:25} {1:3}\".format(name, count), end='   ')\n",
    "    if (i + 1) % 3 == 0:\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mask = np.zeros(people.target.shape, dtype=np.bool)\n",
    "for target in np.unique(people.target):\n",
    "    mask[np.where(people.target == target)[0][:50]] = 1\n",
    "    \n",
    "X_people = people.data[mask]\n",
    "y_people = people.target[mask]\n",
    "\n",
    "# 0~255 사이의 흑백 이미지의 픽셀 값을 0~1 사이로 스케일 조정합니다.\n",
    "# (옮긴이) MinMaxScaler를 적용하는 것과 거의 동일합니다.\n",
    "X_people = X_people / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# 데이터를 훈련 세트와 테스트 세트로 나눕니다\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_people, y_people, stratify=y_people, random_state=0)\n",
    "# 이웃 개수를 한 개로 하여 KNeighborsClassifier 모델을 만듭니다\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X_train, y_train)\n",
    "print(\"1-최근접 이웃의 테스트 세트 점수: {:.2f}\".format(knn.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "mglearn.plots.plot_pca_whitening()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=100, whiten=True, random_state=0).fit(X_train)\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "print(\"X_train_pca.shape: {}\".format(X_train_pca.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X_train_pca, y_train)\n",
    "print(\"테스트 세트 정확도: {:.2f}\".format(knn.score(X_test_pca, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"pca.components_.shape: {}\".format(pca.components_.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 5, figsize=(15, 12),\n",
    "                         subplot_kw={'xticks': (), 'yticks': ()})\n",
    "for i, (component, ax) in enumerate(zip(pca.components_, axes.ravel())):\n",
    "    ax.imshow(component.reshape(image_shape), cmap='viridis')\n",
    "    ax.set_title(\"주성분 {}\".format((i + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "\n",
    "image_shape = people.images[0].shape\n",
    "plt.figure(figsize=(20, 3))\n",
    "ax = plt.gca()\n",
    "\n",
    "imagebox = OffsetImage(people.images[0], zoom=2, cmap=\"gray\")\n",
    "ab = AnnotationBbox(imagebox, (.05, 0.4), pad=0.0, xycoords='data')\n",
    "ax.add_artist(ab)\n",
    "\n",
    "for i in range(4):\n",
    "    imagebox = OffsetImage(pca.components_[i].reshape(image_shape), zoom=2,\n",
    "                           cmap=\"viridis\")\n",
    "\n",
    "    ab = AnnotationBbox(imagebox, (.285 + .2 * i, 0.4),\n",
    "                        pad=0.0, xycoords='data')\n",
    "    ax.add_artist(ab)\n",
    "    if i == 0:\n",
    "        plt.text(.155, .3, 'x_{} *'.format(i), fontdict={'fontsize': 30})\n",
    "    else:\n",
    "        plt.text(.145 + .2 * i, .3, '+ x_{} *'.format(i),\n",
    "                 fontdict={'fontsize': 30})\n",
    "\n",
    "plt.text(.95, .3, '+ ...', fontdict={'fontsize': 30})\n",
    "\n",
    "plt.rc('text')\n",
    "plt.text(.12, .3, '=', fontdict={'fontsize': 30})\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "plt.rc('text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.plots.plot_pca_faces(X_train, X_test, image_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.discrete_scatter(X_train_pca[:, 0], X_train_pca[:, 1], y_train)\n",
    "plt.xlabel(\"첫 번째 주성분\")\n",
    "plt.ylabel(\"두 번째 주성분\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NMF (음수 미포함 행렬 분해)\n",
    "##### 인공 데이터에 NMF 적용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "mglearn.plots.plot_nmf_illustration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 얼굴 이미지에 NMF 적용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.plots.plot_nmf_faces(X_train, X_test, image_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "nmf = NMF(n_components=15, random_state=0)\n",
    "nmf.fit(X_train)\n",
    "X_train_nmf = nmf.transform(X_train)\n",
    "X_test_nmf = nmf.transform(X_test)\n",
    "\n",
    "fig, axes = plt.subplots(3, 5, figsize=(15, 12),\n",
    "                         subplot_kw={'xticks': (), 'yticks': ()})\n",
    "for i, (component, ax) in enumerate(zip(nmf.components_, axes.ravel())):\n",
    "    ax.imshow(component.reshape(image_shape))\n",
    "    ax.set_title(\"성분 {}\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compn = 3\n",
    "# 4번째 성분으로 정렬하여 처음 10개 이미지를 출력합니다\n",
    "inds = np.argsort(X_train_nmf[:, compn])[::-1]\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 8),\n",
    "                         subplot_kw={'xticks': (), 'yticks': ()})\n",
    "for i, (ind, ax) in enumerate(zip(inds, axes.ravel())):\n",
    "    ax.imshow(X_train[ind].reshape(image_shape))\n",
    "    \n",
    "compn = 7\n",
    "# 8번째 성분으로 정렬하여 처음 10개 이미지를 출력합니다\n",
    "inds = np.argsort(X_train_nmf[:, compn])[::-1]\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 8),\n",
    "                         subplot_kw={'xticks': (), 'yticks': ()})\n",
    "for i, (ind, ax) in enumerate(zip(inds, axes.ravel())):\n",
    "    ax.imshow(X_train[ind].reshape(image_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = mglearn.datasets.make_signals()\n",
    "plt.figure(figsize=(6, 1))\n",
    "plt.plot(S, '-')\n",
    "plt.xlabel(\"시간\")\n",
    "plt.ylabel(\"신호\")\n",
    "plt.margins(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본 데이터를 사용해 100개의 측정 데이터를 만듭니다\n",
    "A = np.random.RandomState(0).uniform(size=(100, 3))\n",
    "X = np.dot(S, A.T)\n",
    "print(\"측정 데이터 형태: {}\".format(X.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf = NMF(n_components=3, random_state=42)\n",
    "S_ = nmf.fit_transform(X)\n",
    "print(\"복원한 신호 데이터 형태: {}\".format(S_.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "H = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [X, S, S_, H]\n",
    "names = ['측정 신호 (처음 3개)',\n",
    "         '원본 신호',\n",
    "         'NMF로 복원한 신호', \n",
    "         'PCA로 복원한 신호']\n",
    "\n",
    "fig, axes = plt.subplots(4, figsize=(8, 4), gridspec_kw={'hspace': .5},\n",
    "                         subplot_kw={'xticks': (), 'yticks': ()})\n",
    "\n",
    "for model, name, ax in zip(models, names, axes):\n",
    "    ax.set_title(name)\n",
    "    ax.plot(model[:, :3], '-')\n",
    "    ax.margins(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t-SNE를 이용한 매니폴드 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(10, 5),\n",
    "                         subplot_kw={'xticks':(), 'yticks': ()})\n",
    "for ax, img in zip(axes.ravel(), digits.images):\n",
    "    ax.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA 모델을 생성합니다\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(digits.data)\n",
    "# 처음 두 개의 주성분으로 숫자 데이터를 변환합니다\n",
    "digits_pca = pca.transform(digits.data)\n",
    "colors = [\"#476A2A\", \"#7851B8\", \"#BD3430\", \"#4A2D4E\", \"#875525\",\n",
    "          \"#A83683\", \"#4E655E\", \"#853541\", \"#3A3120\",\"#535D8E\"]\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.xlim(digits_pca[:, 0].min(), digits_pca[:, 0].max())\n",
    "plt.ylim(digits_pca[:, 1].min(), digits_pca[:, 1].max())\n",
    "for i in range(len(digits.data)):\n",
    "    # 숫자 텍스트를 이용해 산점도를 그립니다\n",
    "    plt.text(digits_pca[i, 0], digits_pca[i, 1], str(digits.target[i]),\n",
    "             color = colors[digits.target[i]],\n",
    "             fontdict={'weight': 'bold', 'size': 9})\n",
    "plt.xlabel(\"첫 번째 주성분\")\n",
    "plt.ylabel(\"두 번째 주성분\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(random_state=42)\n",
    "# TSNE에는 transform 메소드가 없으므로 대신 fit_transform을 사용합니다\n",
    "digits_tsne = tsne.fit_transform(digits.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.xlim(digits_tsne[:, 0].min(), digits_tsne[:, 0].max() + 1)\n",
    "plt.ylim(digits_tsne[:, 1].min(), digits_tsne[:, 1].max() + 1)\n",
    "for i in range(len(digits.data)):\n",
    "    # 숫자 텍스트를 이용해 산점도를 그립니다\n",
    "    plt.text(digits_tsne[i, 0], digits_tsne[i, 1], str(digits.target[i]),\n",
    "             color = colors[digits.target[i]],\n",
    "             fontdict={'weight': 'bold', 'size': 9})\n",
    "plt.xlabel(\"t-SNE 특성 0\")\n",
    "plt.xlabel(\"t-SNE 특성 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 군집\n",
    "  * 데이터 셋을 클러스터라는 그룹으로 나누는 작업\n",
    "  * 각 데이터가 어느 클러스터에 속하는 지 할당(예측)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-평균 군집\n",
    "  * 데이터 집합의 클러스터 중심(Cluster center)을 찾음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "mglearn.plots.plot_kmeans_algorithm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "mglearn.plots.plot_kmeans_boundaries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# 인위적으로 2차원 데이터를 생성합니다\n",
    "X, y = make_blobs(random_state=1)\n",
    "\n",
    "# 군집 모델을 만듭니다\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kmeans.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kmeans.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], kmeans.labels_, markers='o')\n",
    "mglearn.discrete_scatter(\n",
    "    kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], [0, 1, 2],\n",
    "    markers='^', markeredgewidth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# 두 개의 클러스터 중심을 사용합니다\n",
    "kmeans = KMeans(n_clusters=2)\n",
    "kmeans.fit(X)\n",
    "assignments = kmeans.labels_\n",
    "\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], assignments, ax=axes[0])\n",
    "\n",
    "# 다섯 개의 클러스터 중심을 사용합니다\n",
    "kmeans = KMeans(n_clusters=5)\n",
    "kmeans.fit(X)\n",
    "assignments = kmeans.labels_\n",
    "\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], assignments, ax=axes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### k-평균 알고리즘이 실패하는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_varied, y_varied = make_blobs(n_samples=200,\n",
    "                                cluster_std=[1.0, 2.5, 0.5],\n",
    "                                random_state=170)\n",
    "y_pred = KMeans(n_clusters=3, random_state=0).fit_predict(X_varied)\n",
    "\n",
    "mglearn.discrete_scatter(X_varied[:, 0], X_varied[:, 1], y_pred)\n",
    "plt.legend([\"클러스터 0\", \"클러스터 1\", \"클러스터 2\"], loc='best')\n",
    "plt.xlabel(\"특성 0\")\n",
    "plt.ylabel(\"특성 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 무작위로 클러스터 데이터 생성합니다\n",
    "X, y = make_blobs(random_state=170, n_samples=600)\n",
    "rng = np.random.RandomState(74)\n",
    "\n",
    "# 데이터가 길게 늘어지도록 변경합니다\n",
    "transformation = rng.normal(size=(2, 2))\n",
    "X = np.dot(X, transformation)\n",
    "\n",
    "# 세 개의 클러스터로 데이터에 KMeans 알고리즘을 적용합니다\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(X)\n",
    "y_pred = kmeans.predict(X)\n",
    "\n",
    "# 클러스터 할당과 클러스터 중심을 나타냅니다\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], kmeans.labels_, markers='o')\n",
    "mglearn.discrete_scatter(\n",
    "    kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], [0, 1, 2],\n",
    "    markers='^', markeredgewidth=2)\n",
    "plt.xlabel(\"특성 0\")\n",
    "plt.ylabel(\"특성 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two_moons 데이터를 생성합니다(이번에는 노이즈를 조금만 넣습니다)\n",
    "from sklearn.datasets import make_moons\n",
    "X, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n",
    "\n",
    "# 두 개의 클러스터로 데이터에 KMeans 알고리즘을 적용합니다\n",
    "kmeans = KMeans(n_clusters=2)\n",
    "kmeans.fit(X)\n",
    "y_pred = kmeans.predict(X)\n",
    "\n",
    "# 클러스터 할당과 클러스터 중심을 표시합니다\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap=mglearn.cm2, s=60, edgecolors='k')\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n",
    "            marker='^', c=[mglearn.cm2(0), mglearn.cm2(1)], s=100, linewidth=2, edgecolors='k')\n",
    "plt.xlabel(\"특성 0\")\n",
    "plt.ylabel(\"특성 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 벡터 양자화 또는 분해 메소드로서의 k-평균"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_people, y_people, stratify=y_people, random_state=0)\n",
    "nmf = NMF(n_components=100, random_state=0)\n",
    "nmf.fit(X_train)\n",
    "pca = PCA(n_components=100, random_state=0)\n",
    "pca.fit(X_train)\n",
    "kmeans = KMeans(n_clusters=100, random_state=0)\n",
    "kmeans.fit(X_train)\n",
    "\n",
    "X_reconstructed_pca = pca.inverse_transform(pca.transform(X_test))\n",
    "X_reconstructed_kmeans = kmeans.cluster_centers_[kmeans.predict(X_test)]\n",
    "X_reconstructed_nmf = np.dot(nmf.transform(X_test), nmf.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 5, figsize=(8, 8), subplot_kw={'xticks': (), 'yticks': ()})\n",
    "fig.suptitle(\"추출한 성분\")\n",
    "for ax, comp_kmeans, comp_pca, comp_nmf in zip(\n",
    "        axes.T, kmeans.cluster_centers_, pca.components_, nmf.components_):\n",
    "    ax[0].imshow(comp_kmeans.reshape(image_shape))\n",
    "    ax[1].imshow(comp_pca.reshape(image_shape), cmap='viridis')\n",
    "    ax[2].imshow(comp_nmf.reshape(image_shape))\n",
    "\n",
    "axes[0, 0].set_ylabel(\"kmeans\")\n",
    "axes[1, 0].set_ylabel(\"pca\")\n",
    "axes[2, 0].set_ylabel(\"nmf\")\n",
    "\n",
    "fig, axes = plt.subplots(4, 5, subplot_kw={'xticks': (), 'yticks': ()},\n",
    "                         figsize=(8, 8))\n",
    "fig.suptitle(\"재구성\")\n",
    "for ax, orig, rec_kmeans, rec_pca, rec_nmf in zip(\n",
    "        axes.T, X_test, X_reconstructed_kmeans, X_reconstructed_pca,\n",
    "        X_reconstructed_nmf):\n",
    "    \n",
    "    ax[0].imshow(orig.reshape(image_shape))\n",
    "    ax[1].imshow(rec_kmeans.reshape(image_shape))\n",
    "    ax[2].imshow(rec_pca.reshape(image_shape))\n",
    "    ax[3].imshow(rec_nmf.reshape(image_shape))\n",
    "\n",
    "axes[0, 0].set_ylabel(\"원본\")\n",
    "axes[1, 0].set_ylabel(\"kmeans\")\n",
    "axes[2, 0].set_ylabel(\"pca\")\n",
    "axes[3, 0].set_ylabel(\"nmf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n",
    "\n",
    "kmeans = KMeans(n_clusters=10, random_state=0)\n",
    "kmeans.fit(X)\n",
    "y_pred = kmeans.predict(X)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred, s=60, cmap='Paired', edgecolors='black')\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=60,\n",
    "            marker='^', c=range(kmeans.n_clusters), linewidth=2, cmap='Paired', edgecolors='black')\n",
    "plt.xlabel(\"특성 0\")\n",
    "plt.ylabel(\"특성 1\")\n",
    "print(\"클러스터 레이블:\\n{}\".format(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_features = kmeans.transform(X)\n",
    "print(\"클러스터 거리 데이터의 형태: {}\".format(distance_features.shape))\n",
    "print(\"클러스터 거리:\\n{}\".format(distance_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 병합 군집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "mglearn.plots.plot_agglomerative_algorithm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "X, y = make_blobs(random_state=1)\n",
    "\n",
    "agg = AgglomerativeClustering(n_clusters=3)\n",
    "assignment = agg.fit_predict(X)\n",
    "\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], assignment)\n",
    "plt.legend([\"클러스터 0\", \"클러스터 1\", \"클러스터 2\"], loc=\"best\")\n",
    "plt.xlabel(\"특성 0\")\n",
    "plt.ylabel(\"특성 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 계층적 군집과 덴드로그램(dendrograms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "mglearn.plots.plot_agglomerative()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SciPy에서 ward 군집 함수와 덴드로그램 함수를 임포트합니다\n",
    "from scipy.cluster.hierarchy import dendrogram, ward\n",
    "\n",
    "X, y = make_blobs(random_state=0, n_samples=12)\n",
    "# 데이터 배열 X 에 ward 함수를 적용합니다\n",
    "# SciPy의 ward 함수는 병합 군집을 수행할 때 생성된\n",
    "# 거리 정보가 담긴 배열을 리턴합니다\n",
    "linkage_array = ward(X)\n",
    "# 클러스터 간의 거리 정보가 담긴 linkage_array를 사용해 덴드로그램을 그립니다\n",
    "dendrogram(linkage_array)\n",
    "\n",
    "# 두 개와 세 개의 클러스터를 구분하는 커트라인을 표시합니다\n",
    "ax = plt.gca()\n",
    "bounds = ax.get_xbound()\n",
    "ax.plot(bounds, [7.25, 7.25], '--', c='k')\n",
    "ax.plot(bounds, [4, 4], '--', c='k')\n",
    "\n",
    "ax.text(bounds[1], 7.25, ' 두 개 클러스터', va='center', fontdict={'size': 15})\n",
    "ax.text(bounds[1], 4, ' 세 개 클러스터', va='center', fontdict={'size': 15})\n",
    "plt.xlabel(\"샘플 번호\")\n",
    "plt.ylabel(\"클러스터 거리\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "X, y = make_blobs(random_state=0, n_samples=12)\n",
    "\n",
    "dbscan = DBSCAN()\n",
    "clusters = dbscan.fit_predict(X)\n",
    "print(\"클러스터 레이블:\\n{}\".format(clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mglearn.plots.plot_dbscan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n",
    "\n",
    "# 평균이 0, 분산이 1이 되도록 데이터의 스케일을 조정합니다\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "dbscan = DBSCAN()\n",
    "clusters = dbscan.fit_predict(X_scaled)\n",
    "# 클러스터 할당을 표시합니다\n",
    "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=clusters, cmap=mglearn.cm2, s=60, edgecolors='black')\n",
    "plt.xlabel(\"특성 0\")\n",
    "plt.ylabel(\"특성 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### 군집 알고리즘의 비교와 평가\n",
    "##### 타겟값으로 군집 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "X, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n",
    "\n",
    "# 평균이 0, 분산이 1이 되도록 데이터의 스케일을 조정합니다\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(15, 3),\n",
    "                         subplot_kw={'xticks': (), 'yticks': ()})\n",
    "\n",
    "# 사용할 알고리즘 모델을 리스트로 만듭니다\n",
    "algorithms = [KMeans(n_clusters=2), AgglomerativeClustering(n_clusters=2),\n",
    "              DBSCAN()]\n",
    "\n",
    "# 비교를 위해 무작위로 클러스터 할당을 합니다\n",
    "random_state = np.random.RandomState(seed=0)\n",
    "random_clusters = random_state.randint(low=0, high=2, size=len(X))\n",
    "\n",
    "# 무작위 할당한 클러스터를 그립니다\n",
    "axes[0].scatter(X_scaled[:, 0], X_scaled[:, 1], c=random_clusters,\n",
    "                cmap=mglearn.cm3, s=60, edgecolors='black')\n",
    "axes[0].set_title(\"무작위 할당 - ARI: {:.2f}\".format(\n",
    "        adjusted_rand_score(y, random_clusters)))\n",
    "\n",
    "for ax, algorithm in zip(axes[1:], algorithms):\n",
    "    # 클러스터 할당과 클러스터 중심을 그립니다\n",
    "    clusters = algorithm.fit_predict(X_scaled)\n",
    "    ax.scatter(X_scaled[:, 0], X_scaled[:, 1], c=clusters,\n",
    "               cmap=mglearn.cm3, s=60, edgecolors='black')\n",
    "    ax.set_title(\"{} - ARI: {:.2f}\".format(algorithm.__class__.__name__,\n",
    "                                           adjusted_rand_score(y, clusters)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 포인트가 클러스터로 나뉜 두 가지 경우\n",
    "clusters1 = [0, 0, 1, 1, 0]\n",
    "clusters2 = [1, 1, 0, 0, 1]\n",
    "# 모든 레이블이 달라졌으므로 정확도는 0입니다\n",
    "print(\"정확도: {:.2f}\".format(accuracy_score(clusters1, clusters2)))\n",
    "# 같은 포인트가 클러스터에 모였으므로 ARI는 1입니다\n",
    "print(\"ARI: {:.2f}\".format(adjusted_rand_score(clusters1, clusters2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 타겟값 없이 군집 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import silhouette_score\n",
    "\n",
    "X, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n",
    "\n",
    "# 평균이 0, 분산이 1이 되도록 데이터의 스케일을 조정합니다\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(15, 3),\n",
    "                         subplot_kw={'xticks': (), 'yticks': ()})\n",
    "\n",
    "# 비교를 위해 무작위로 클러스터 할당을 합니다\n",
    "random_state = np.random.RandomState(seed=0)\n",
    "random_clusters = random_state.randint(low=0, high=2, size=len(X))\n",
    "\n",
    "# 무작위 할당한 클러스터를 그립니다\n",
    "axes[0].scatter(X_scaled[:, 0], X_scaled[:, 1], c=random_clusters,\n",
    "                cmap=mglearn.cm3, s=60, edgecolors='black')\n",
    "axes[0].set_title(\"무작위 할당: {:.2f}\".format(\n",
    "        silhouette_score(X_scaled, random_clusters)))\n",
    "\n",
    "algorithms = [KMeans(n_clusters=2), AgglomerativeClustering(n_clusters=2),\n",
    "              DBSCAN()]\n",
    "\n",
    "for ax, algorithm in zip(axes[1:], algorithms):\n",
    "    clusters = algorithm.fit_predict(X_scaled)\n",
    "    # 클러스터 할당과 클러스터 중심을 그립니다\n",
    "    ax.scatter(X_scaled[:, 0], X_scaled[:, 1], c=clusters, cmap=mglearn.cm3,\n",
    "               s=60, edgecolors='black')\n",
    "    ax.set_title(\"{} : {:.2f}\".format(algorithm.__class__.__name__,\n",
    "                                      silhouette_score(X_scaled, clusters)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 얼굴 데이터셋으로 군집 알고리즘 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LFW 데이터에서 고유얼굴을 찾은 다음 데이터를 변환합니다\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=100, whiten=True, random_state=0)\n",
    "pca.fit_transform(X_people)\n",
    "X_pca = pca.transform(X_people)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DBSCAN으로 얼굴 데이터셋 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 매개변수로 DBSCAN을 적용합니다\n",
    "dbscan = DBSCAN()\n",
    "labels = dbscan.fit_predict(X_pca)\n",
    "print(\"고유한 레이블: {}\".format(np.unique(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(min_samples=3)\n",
    "labels = dbscan.fit_predict(X_pca)\n",
    "print(\"고유한 레이블: {}\".format(np.unique(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(min_samples=3, eps=15)\n",
    "labels = dbscan.fit_predict(X_pca)\n",
    "print(\"고유한 레이블: {}\".format(np.unique(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 잡음 포인트와 클러스터에 속한 포인트 수를 셉니다.\n",
    "# bincount는 음수를 받을 수 없어서 labels에 1을 더했습니다.\n",
    "# 반환값의 첫 번째 원소는 잡음 포인트의 수입니다.\n",
    "print(\"클러스터별 포인트 수: {}\".format(np.bincount(labels + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = X_people[labels==-1]\n",
    "\n",
    "fig, axes = plt.subplots(3, 9, subplot_kw={'xticks': (), 'yticks': ()},\n",
    "                         figsize=(12, 4))\n",
    "for image, ax in zip(noise, axes.ravel()):\n",
    "    ax.imshow(image.reshape(image_shape), vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for eps in [1, 3, 5, 7, 9, 11, 13]:\n",
    "    print(\"\\neps={}\".format(eps))\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=3)\n",
    "    labels = dbscan.fit_predict(X_pca)\n",
    "    print(\"클러스터 수: {}\".format(len(np.unique(labels))))\n",
    "    print(\"클러스터 크기: {}\".format(np.bincount(labels + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(min_samples=3, eps=7)\n",
    "labels = dbscan.fit_predict(X_pca)\n",
    "\n",
    "for cluster in range(max(labels) + 1):\n",
    "    mask = labels == cluster\n",
    "    n_images =  np.sum(mask)\n",
    "    fig, axes = plt.subplots(1, 14, figsize=(14*1.5, 4),\n",
    "                             subplot_kw={'xticks': (), 'yticks': ()})\n",
    "    i = 0\n",
    "    for image, label, ax in zip(X_people[mask], y_people[mask], axes):\n",
    "        ax.imshow(image.reshape(image_shape), vmin=0, vmax=1)\n",
    "        ax.set_title(people.target_names[label].split()[-1])\n",
    "        i += 1\n",
    "    for j in range(len(axes) - i):\n",
    "        axes[j+i].imshow(np.array([[1]*65]*87), vmin=0, vmax=1)\n",
    "        axes[j+i].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### k-평균으로 얼굴 데이터셋 분석하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 10\n",
    "# k-평균으로 클러스터를 추출합니다\n",
    "km = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "labels_km = km.fit_predict(X_pca)\n",
    "print(\"k-평균의 클러스터 크기: {}\".format(np.bincount(labels_km)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 5, subplot_kw={'xticks': (), 'yticks': ()},\n",
    "                         figsize=(12, 4))\n",
    "for center, ax in zip(km.cluster_centers_, axes.ravel()):\n",
    "    ax.imshow(pca.inverse_transform(center).reshape(image_shape),\n",
    "              vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.plots.plot_kmeans_faces(km, pca, X_pca, X_people,\n",
    "                                y_people, people.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 병합 군집으로 얼굴 데이터셋 분석하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 병합 군집으로 클러스터를 추출합니다\n",
    "agglomerative = AgglomerativeClustering(n_clusters=10)\n",
    "labels_agg = agglomerative.fit_predict(X_pca)\n",
    "print(\"병합 군집의 클러스터 크기: {}\".format(\n",
    "       np.bincount(labels_agg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ARI: {:.2f}\".format(adjusted_rand_score(labels_agg, labels_km)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkage_array = ward(X_pca)\n",
    "# 클러스터 사이의 거리가 담겨있는 linkage_array로 덴드로그램을 그립니다\n",
    "plt.figure(figsize=(20, 5))\n",
    "dendrogram(linkage_array, p=7, truncate_mode='level', no_labels=True)\n",
    "plt.xlabel(\"샘플 번호\")\n",
    "plt.ylabel(\"클러스터 거리\")\n",
    "ax = plt.gca()\n",
    "bounds = ax.get_xbound()\n",
    "ax.plot(bounds, [36, 36], '--', c='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_clusters = 10\n",
    "for cluster in range(n_clusters):\n",
    "    mask = labels_agg == cluster\n",
    "    fig, axes = plt.subplots(1, 10, subplot_kw={'xticks': (), 'yticks': ()},\n",
    "                             figsize=(15, 8))\n",
    "    axes[0].set_ylabel(np.sum(mask))\n",
    "    for image, label, asdf, ax in zip(X_people[mask], y_people[mask],\n",
    "                                      labels_agg[mask], axes):\n",
    "        ax.imshow(image.reshape(image_shape), vmin=0, vmax=1)\n",
    "        ax.set_title(people.target_names[label].split()[-1],\n",
    "                     fontdict={'fontsize': 9})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 병합 군집으로 클러스터를 추출합니다\n",
    "agglomerative = AgglomerativeClustering(n_clusters=40)\n",
    "labels_agg = agglomerative.fit_predict(X_pca)\n",
    "print(\"병합 군집의 클러스터 크기: {}\".format(np.bincount(labels_agg)))\n",
    "\n",
    "n_clusters = 40\n",
    "for cluster in [13, 16, 23, 38, 39]: # 흥미로운 클러스터 몇개를 골랐습니다\n",
    "    mask = labels_agg == cluster\n",
    "    fig, axes = plt.subplots(1, 15, subplot_kw={'xticks': (), 'yticks': ()},\n",
    "                             figsize=(15, 8))\n",
    "    cluster_size = np.sum(mask)\n",
    "    axes[0].set_ylabel(\"#{}: {}\".format(cluster, cluster_size))\n",
    "    for image, label, asdf, ax in zip(X_people[mask], y_people[mask],\n",
    "                                      labels_agg[mask], axes):\n",
    "        ax.imshow(image.reshape(image_shape), vmin=0, vmax=1)\n",
    "        ax.set_title(people.target_names[label].split()[-1],\n",
    "                     fontdict={'fontsize': 9})\n",
    "    for i in range(cluster_size, 15):\n",
    "        axes[i].set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 군집 알고리즘 요약"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 요약 및 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
